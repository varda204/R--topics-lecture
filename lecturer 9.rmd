---
title: "R Notebook"
output: html_notebook
---
## Sample Statistics

```{r}
data<-c(0.9,1.3,1.8,2.5,2.6,2.8,3.6,4.0,4.1,4.2,4.3,4.3,4.6,4.6,4.6,4.7,4.8,4.9,4.9,5.0)
mean(data)
median(data)
length(data)
```

```{r}
hist(data)
```


```{r}
data_sort <- sort(data)
```

```{r}
data_sort <- data_sort[-20]
data_sort <- data_sort[-19]
data_sort <- data_sort[-2]
data_sort <- data_sort[-1]
print(data_sort)
mean(data_sort)
```
```{r}
summary(data)
```

```{r}
quantile(data, probs = seq(0, 1, 0.25), na.rm = FALSE, names = TRUE)
```
```{r}
boxplot(data)
```

##barplot

```{r}
plot((airquality$Ozone))
boxplot(airquality$Ozone,horizontal = TRUE)
```

```{r}
boxplot(airquality$Ozone,
main = "Mean ozone in parts per billion at Roosevelt Island",
xlab = "Parts Per Billion",
ylab = "Ozone",
col = "orange",
border = "brown",
horizontal = TRUE,
notch = TRUE
)
```


## Central limit theorem

The Central limit theorem states that the sampling distribution of the mean of any independent, random variable will be normal or near normal,regardless of underlying distribution.If the sample size is large enough,we get a nice bell shaped curve.

In other words, suppose you picked a sample from large number of independent and random observations and compute the arithmetic average of sample and do this exercise for n number of times. Then according to the central limit theorem the computed values of the average will be distributed according to the normal distribution (commonly known as a “bell curve”). We will try simulate this my theorem in examples below

Example 1: A fair die can be modelled with a discrete random variable with outcome 1 through 6, each with the equal probability of 1/6.

The expected value is 1+2+3+4+5+6 =3.5
Suppose you throw the die 10000 times and plot the frequency of each outcome. Here’s the r syntax to simulate the throwing a die 10000 times
```{r}
DieOutcome <- sample(1:6,10000, replace= TRUE)
hist(DieOutcome, col ="light blue")
abline(v=3.5, col = "red",lty=1)
```

We will take samples of size 10 , from the above 10000 observation of outcome of die roll, take the arithmetic mean and try to plot the mean of sample. we will do this procedure k times (in this case k= 10000 )
```{r}
x10 <- c()
k =10000
 for ( i in 1:k) {
 x10[i] = mean(sample(1:6,10, replace = TRUE))}
 hist(x10, col ="pink", main="Sample size =10",xlab ="Outcome of die roll")
 abline(v = mean(x10), col = "Red")
 abline(v = 3.5, col = "blue")
```

Sample Size
By theory , we know as the sample increases, we get better bell shaped curve. As the n apporaches infinity , we get a normal distribution. Lets do this by increasing the sample size to 30, 100 and 1000 in above example 1.

```{r}
 x1 <- c()
 x5 <- c()
 x10 <- c()
 x20 <- c()
 k =1000
 for (i in 1:k){
 x1[i] = mean(sample(1:2,1, replace = TRUE))
 x5[i] = mean(sample(1:2,5, replace = TRUE))
 x10[i] = mean(sample(1:2,10, replace = TRUE))
 x20[i] = mean(sample(1:2,20, replace = TRUE))
 }
 par(mfrow=c(1,4))
hist(x1, col ="orange",main="n=1",xlab ="die roll")
 abline(v = mean(x1), col = "red")
 
 hist(x5, col ="green",main="n=5",xlab ="die roll")
 abline(v = mean(x5), col = "red")
 
 hist(x10, col ="light blue", main="n=10",xlab ="die roll")
 abline(v = mean(x10), col = "red")
 
 hist(x20, col ="red",main="n=20",xlab ="die roll")
 abline(v = mean(x20), col = "blue")
```

```{r}
 x30 <- c()
 x100 <- c()
 x1000 <- c()
 k =10000
 for (i in 1:k){
 x30[i] = mean(sample(1:6,30, replace = TRUE))
 x100[i] = mean(sample(1:6,100, replace = TRUE))
 x1000[i] = mean(sample(1:6,1000, replace = TRUE))
 }
 par(mfrow=c(1,3))
 hist(x30, col ="green",main="n=30",xlab ="die roll")
 abline(v = mean(x30), col = "blue")

 hist(x100, col ="light blue", main="n=100",xlab ="die roll")
 abline(v = mean(x100), col = "red")

 hist(x1000, col ="orange",main="n=1000",xlab ="die roll")
 abline(v = mean(x1000), col = "red")
```

We will take another example

Example 2: A fair Coin
Flipping a fair coin many times the probability of getting a given number of heads in a series of flips should follow a normal curve, with mean equal to half the total number of flips in each series. Here 1 represent heads and 0 tails.
```{r}
x <- c()
k =10000  
 for ( i in 1:k) {  
 x[i] = mean(sample(0:1,100, replace = TRUE))}  
 hist(x, col ="light green", main="Sample size = 100",xlab ="flipping coin ")  
 abline(v = mean(x), col = "red")
```

## Point Estimates
Point estimates are estimates of population parameters based on sample data. For instance, if we wanted to know the average age of registered voters in the U.S., we could take a survey of registered voters and then use the average age of the respondents as a point estimate of the average age of the population as a whole. The average of a sample is known as the sample mean.
The sample mean is usually not exactly the same as the population mean. This difference can be caused by many factors including poor survey design, biased sampling methods and the randomness inherent to drawing a sample from a population. Let's investigate point estimates by generating a population of random age data and then drawing a sample from it to estimate the mean:
In [1]:
```{r}
set.seed(12)
population_ages <- c(rexp(1000000,0.015)+18,   # Generate a population
                    rpois(500000,20)+18,
                    rpois(500000,32.5)+18,
                    rpois(500000,45)+18)
population_ages <- ifelse(population_ages<100, population_ages, population_ages%%100+18)
true_mean <- mean(population_ages)            # Check the population mean
true_mean
```
```{r}
set.seed(7)
sample_ages <- sample(population_ages, size=1000)  # Take a sample of 1000 ages
sample_mean <- mean(sample_ages)            # Make a point estimate of the mean
sample_mean
sample_mean-true_mean   # Check difference between estimate and population parameter
```
Our point estimate based on a sample of 1000 individuals overestimates the true population mean by almost a year, but it is pretty close. This illustrates an important point: we can get a fairly accurate estimate of a large population by sampling a relatively small subset of individuals.
Another point estimate that may be of interest is the proportion of the population that belongs to some category or subgroup. For example, we might like to know the race of each voter we poll, to get a sense of the overall demographics of the voter base. You can make a point estimate of this sort of proportion by taking a sample and then checking the ratio in the sample:
```{r}
set.seed(12)
population_races <- c(rep("white",1000000),    # Generate some dummy demographic data
                      rep("hispanic",500000),
                      rep("black",500000),
                      rep("asian",250000),
                      rep("other",250000))

demographic_sample <- sample(population_races, size=1000)       # Take a sample
for (race in unique(demographic_sample)){            # Loop over each race*
    print(paste(race," proportion estimate:"))  
# Print the estimated name
a=sum(demographic_sample==race)/1000 
# Print the estimated proportion
b=length(which(population_races==race))/length(population_races)
a
b
bias=a-b
print(a)
print(b)
print(a-b)
}
```

*Note: The function unique() takes a vector and returns a new vector with duplicate elements removed.

